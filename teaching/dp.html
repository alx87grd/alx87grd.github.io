<!DOCTYPE html>
<html>
<head>
<title>Apprentissage par renforcement et commande optimale</title>
<meta charset="UTF-8">
<meta name="author" content="Alexandre Girard, Sherbrooke, Québec, Canada" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>


<!-- Navbar (sit on top) -->
<div class="w3-top">
  <div class="w3-bar w3-white w3-wide w3-padding w3-card">
    <a href="index.html" class="w3-bar-item w3-button"><b>Alex</b> Robotics</a>
    <!-- Float links to the right. Hide them on small screens 
    <div class="w3-right w3-hide-small">
      <a href="index.html" class="w3-bar-item w3-button">Button 1</a>
    </div>
  -->
  </div>
</div>

<!-- Header -->
<!-- <header class="w3-display-container w3-content w3-wide" style="max-width:1500px;" id="home">
  <img class="w3-image" src="../IMG/bg3.jpg" alt="sky" width="1500" height="800">
  <div class="w3-display-middle w3-margin-top w3-center">
    <h1 class="w3-xxlarge w3-text-white"><span class="w3-padding w3-black w3-opacity-min"><b>GR0860</b></span> <span class="w3-hide-small w3-text-light-grey"> Apprentissage et commande </span></h1>
  </div>
</header> -->
<!-- Header -->
<header class="w3-display-container w3-content w3-wide" style="max-width:1500px;" id="home">
  <img class="w3-image" src="../IMG/bg3.jpg" alt="sky" width="1500" height="800">
  <div class="w3-display-middle w3-margin-top w3-center">
    <h1 class="w3-xxlarge w3-text-white"><span class="w3-padding w3-black w3-opacity-min"><b>GMC860</b></span> <span class="w3-hide-small w3-text-light-grey">Apprentissage par renforcement et commande optimale</span></h1>
  </div>
</header>

<!-- Page content -->
<div class="w3-content w3-padding" style="max-width:1564px">

  <div class="w3-container w3-padding-16" id="preface">

    <h1 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Apprentissage par renforcement et commande optimale
    </h1>
    
    <div class="w3-col l6 m12 w3-margin-bottom">
      <div class="w3-display-container">

        <h3>
          Pourquoi ce cours?
        </h3>
        <p> 
          <ul>
            <li>Comprendre les fondements de l’apprentissage par renforcement et la commande optimale</li>
            <li>Faire les liens avec la science des asservissements;</li>
            <li>Apprendre à utiliser des algorithmes pour synthétiser des politiques optimales</li>
          </ul>
        </p>

        <h5>
          <em>Du choix des forces dans un robot jusqu'au choix de la pièce à déplacer dans un jeu d'échec.</em>
        </h5>

        <h3>
          Quoi?
        </h3>

        <p> 
          <ul>
            <li>Principe d’optimalité: Équations de Bellman, fonction coût/récompense, contraintes, etc.
            </li>
            <li>Modèle d’évolution: Équations différentielles, Chaînes de Markov, Échantillonnage.
            </li>
            <li>Techniques de solution: Programmation dynamique, LQR, Apprentissage par Renforcement.
            </li>
          </ul>
        </p>

        <h3>
          Quand et où?
        </h3>
        <p> 
          Cours à option à l'université de sherbrooke : <a href="https://www.usherbrooke.ca/admission/fiches-cours/GRO860/apprentissage-par-renforcement-et-commande-optimale/" ><b>GRO860</b> </a>. La version automne 2024 sera les lundi 13h-16h en salle à déterminer.
        </p>

      </div>
    </div>
    

  </div>

    <div class="w3-col l6 m6 w3-margin-bottom">
      

      <h3> 
        Une approche unifiée pour la science de la prise de decision en temps réel.
      </h3>
    
      <p> 
        Le but du cours est de faire le lien entre le domaine des asservissements et les algorithmes de décision basé sur l’IA. Le cours présentera les outils pour vous permettre de de traduire un problème de décisions en temps réel sous la représentation mathématique adapté pour synthétiser et optimiser une politique de décision, avec des applications dans plusieurs domaines de la robotique à la finance.
      </p>

      <div class="w3-display-container">
        <div class="w3-display-topleft w3-black w3-padding">
          Vidéo de présentation du cours
        </div>
          <iframe style="width:100%" height="400"  src="https://www.youtube.com/embed/tbuEGkAQ1Lk?si=Uc6Y41mjVdl_umwZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>

    </div>

  <div class="w3-row-padding">

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Capsules vidéos
     </h2>

    <div class="w3-col l3 m3 w3-margin-bottom">
      <div class="w3-display-container">
          <p> Une série de capsules vidéos associés est disponible au lien à droite:
          </p>
      </div>
    </div>

    <div class="w3-col l9 m9 w3-margin-bottom">
      <div class="w3-display-container">
        <iframe style="width:100%" height="400" src="https://www.youtube.com/embed/videoseries?list=PL6adNeJ0A8UtNslNQfAHAzcjHcQixBSnu" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>
    </div>

  </div>

  <div class="w3-container w3-padding-16">

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Introduction
    </h2>

    <p> 
      Ce cours présente les approches pour prendre des décisions intelligentes sous un cadre théorique unifié basé sur le principe de la programmation dynamique. Il vise d'abord a établir les liens entre les approches issues du domaine de l'ingénierie (la science des asservissements et la commande optimale) et les approches issues des sciences informatiques (recherche opérationnelle et l'apprentissage par renforcement) qui ont en fait les même bases mathématiques. Il vise principalement à donner à un lecteur issue du domaine de l'ingénierie les bases pour comprendre et utiliser les approches numériques issues des sciences informatiques, comme l'apprentissage par renforcement, qui permettent de calculer des politiques décisionnelles optimales.
    </p>

    <p>Plusieurs problèmes en apparence très différents:</p>
    <div class="w3-display-container">
      <img src="../IMG/dp_app.png" style="width:100%; height:280px; object-fit: cover">
    </div>

    <p>sont en fait des problèmes qu'on peut analyser et résoudre avec les mêmes outils mathématiques:</p>
    <div class="w3-display-container">
      <img src="../IMG/bellman_joke.png" style="width:100%; height:280px; object-fit: cover">
    </div>

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Plan du cours
    </h2>

    <h3> Partie 1 - Notions de base avec des cours magistraux (50%): </h3>
    
    <p>
      La première partie comportera des leçons magistrales qui présenteront les notions de base. L’emphase sera sur la compréhension des équations fondamentales illustrées avec plusieurs exemples dans des domaines variés. La première partie comportera 3 devoirs et se terminera à mi-session.
    </p>  

    <h3> Partie 2 – Projet au choix de l’étudiant (50%) : </h3>

    <p>
      La seconde partie comportera des séminaires introduisant aux méthodes avancées et du mentorat pour le travail sur le projet. La seule évaluation sera un projet de session avec un sujet qui sera au choix des étudiants et pourra porter sur une problématique provenant d’un projet de recherche, un projet de fin de baccalauréat, etc.
    </p>  

    <h3 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Contenu Détaillé
    </h3>
  
    <table class="w3-table w3-border">
      <tr>
        <th>Semaines</th>
        <th>Sujets</th>
        <th>Travaux</th>
      </tr>

      <tr>
        <td>TBD</td>
        <td>
          <h4> Introduction </h4> 
          <ul >
            <li> Programmation dynamique </li>
            <li> Applications et exemples (contrôle, navigation, finance, game AI, etc.) </li>
          </ul>
        </td>
        
      </tr>
      <tr>
        <td>TBD</td>
        <td>
          <h4> Principe d’optimalité </h4> 
          <ul>
            <li>Fonction de coût, contraintes et objectifs</li>
            <li>Équations de Bellman</li>
            <li>Équation HJB</li>
            <li>Solution analytique pour les systèmes linéaires avec un coût quadratique (LQR)</li>
            <li>Formulation MiniMax pour le contrôle robuste</li>
            <li>Formulation Probabiliste</li>
          </ul>
        </td>
      </tr>
      <tr>
        <td>TBD</td>
        <td>
          <h4> Modélisation </h4> 
          <ul>
            <li>États discrets vs états continus</li>
            <li>Temps discret vs temps continus</li>
            <li>Évolution déterministe vs évolution stochastique</li>
            <li>Modèle d’état; Équation différentielles; Équation de différences; Chaînes de Markov;</li>
            <li>Graphiques de transitions;</li>
          </ul>
        </td>
      </tr>

      <tr>
        <td>TBD</td>
        <td>
          <h4> Méthode de synthèse / apprentissage </h4> 
          <ul>
            <li>Méthodes hors-ligne</li>
            <ul>
              <li>Itération de valeur (value iteration)</li>
              <li>Itération de loi de commande (policy iteration)</li>
            </ul>
            <li>Apprentissage par renforcement (méthodes adaptative)</li>
            <ul>
              <li>Différence temporelle</li>
              <li>Q-Learning</li>
              <li>Policy gradient</li>
            </ul>
            <li>Méthode en-ligne avec heuristiques</li>
            <ul>
              <li>Rollout</li>
              <li>MPC</li>
            </ul>
          </ul>
        </td>
      </tr>

      <tr>
        <td>TBD</td>
        <td>
          <h4> Méthodes avancées </h4> 
          <ul >
            <li>Approximation de fonctions</li>
            <li>Deep-reinforcement learning</li>
          </ul>
        </td>
        <td>
          <h4> Projet de session  </h4> 
          <ul >
            <li>Un projet au choix de l'étudiant impliquant des notions du cours. </li>
            <li>L’évaluation du projet consistera en une présentation finale devant la classe </li>
          </ul>
        </td>
      </tr>
    </table>

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Programmation dynamique
    </h2>

    <p> 
      La programmation dynamique est un principe mathématique pour optimiser des décisions qui sont prises en séquence après avoir observé l'état d'un système. Le principe peut être utilisé autant pour analyser un système asservis classique, comme contrôler la position d'un moteur en choisissant la tension appliquée à ses bornes, que pour des problèmes probabiliste dans un contexte de finance, comme choisir quand acheter ou vendre une action en observant l'évolution de son prix, ou bien un problème d'IA comme choisir la pièce à déplacer lors d'une partie d'échec en observant la position des pièces sur l'échiquier.
    </p>

    

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Formulation du problème
    </h2>


    <h3> Un comportement défini par une loi de commande à concevoir</h3>

    <p> 
      On s'intéresse au problème de concevoir une loi de commande, qui consiste en une fonction $c$ qui détermine l'action $u$ à prendre en fonction de l'état $x$ d'un système observé, et possiblement du temps $t$:
      
      \begin{align}
      u = c( x, t )
      \end{align}
      
      Cette loi de commande va être implémenté dans un agent qui observe l'état $x$ d'un système et agit en conséquence sur ce système pour l'influencer, formant ainsi une boucle comme illustré à la figure XXX. 
      
    </p> 

    <h3>Un objectif formulé comme la minimisation d'un coût</h3>

    <p> 
    Ensuite, notre loi de commande sera conçue de sorte à atteindre un objectif qui sera exprimé mathématiquement comme la minimisation d'un coût additif qui dépend de la trajectoire du système et les actions utilisée:

      \begin{align}
          J=\int_0^{t_f}g(x,u,t) dt + h(x_f,t_f)
      \end{align}


      où $g$ est un coût cumulatif sur la trajectoire du système, $h$ est un coût terminal et $t_f$ est un horizon de temps. La forme cumulative de la fonction coût est centrale pour utiliser le principe de la programmation dynamique, mais ce n'est pas vraiment restrictif car pratiquement tout les objectifs peuvent être formulés comme la minimisation d'une fonction de coût avec cette forme. Lorsque que notre agent prend les meilleurs décisions possible en fonction de l'objectif on dira que sa loi de commande est optimale au sens qu'elle minimise la fonction de coût. 
    </p>

    <div class="w3-row-padding">

      <div class="w3-col l6 m6 w3-margin-bottom">
        <div class="w3-display-container">
          <div class="w3-display-topleft w3-black w3-padding">
            Coût à venir
          </div>
          <img src="../IMG/cost2go_animation.gif" style="width:100%; height:250px; object-fit: cover">
        </div>
      </div>
  
      <div class="w3-col l6 m6 w3-margin-bottom">
        <div class="w3-display-container">
          <div class="w3-display-topleft w3-black w3-padding">
            Politique optimale
          </div>
          <img src="../IMG/policy_animation.gif" style="width:100%; height:250px; object-fit: cover">
        </div>
      </div>
  
    </div>
    
    <h3>Minimiser un coût ou maximiser une récompense?</h3>
    
    <p>

      On peut alternativement formuler l'objectif comme le problème de maximiser une récompense. Les deux approches sont équivalentes et interchangeables. Typiquement le domaine de la commande optimale utilise la formulation de minimiser un coût, qui est souvent relié à l'erreur par rapport à une trajectoire cible. Alternativement, le domaine de l'apprentissage par renforcement préfère optimiser une récompense, qui est souvent par exemple le pointage dans un jeux pour lequel une IA est développé.

    </p>

    

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16">  
    Exemples de contextes 
    </h2>


    <h3>Loi de commande pour un robot</h3>

    <p>
      Un exemple d'asservissement classique serait un bras robotique où l'action $u$ déterminée par la loi de commande correspond à un vecteur de couples à appliquer dans les moteurs électriques. Cette action sera calculée en fonction de l'état actuel du robot, donc ici un vecteur de positions et vitesses de ses diverses articulations. L'objectif serait formulé comme la minimisation de l'erreur de position du robot par rapport à une position cible et potentiellement d'une pénalité pour utiliser beaucoup d'énergie. Typiquement notre solution de loi de commande serait ici une équation analytique.
    </p>
       
    <h3>Algorithme de navigation</h3>
    
    <p>
      Un exemple de prise de décision à plus haut niveau serait de choisir un trajet sur une carte. La loi de commande déterminerait ici quelle direction prendre en fonction de la position actuelle sur la carte. L'objectif d'atteindre la destination le plus rapidement possible pourrait être formuler comme la minimisation du temps écoulé avant d'atteindre celle-ci. La loi de commande (qui serait une solution globale) pourrait être sous la forme d'une table de correspondance où est en mémoire la direction optimale à prendre pour chaque intersection sur laquelle on peut se trouver sur la carte.
    </p>
       
    <h3>Algorithme d'investissement</h3>
    
    <p>
      Un exemple dans un tout autre contexte serait pour un algorithme d'investissement. L'action de la loi de commande serait ici d'acheter ou non une action en fonction d'une observation de son prix. L'objectif pourrait ici être formuler comme la maximisation des gains financiers. La loi de commande serait ici un seuil de prix, qui pourrait varier en fonction du temps, en dessous duquel l'agent décide d'acheter l'action.
    </p>


    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Formulation en temps discret
    </h2>

    <p>
      La plupart des outils pour travailler avec ce genre de problèmes sont mieux adapté à une approche de type temps discret. Ces notes vont donc présenter les principes et les algorithmes d'abord avec une approche à temps discret ou un index $k$ indique l'étape actuelle. Il est possible de convertir une problème à temps continu en problème à temps discret en introduisant un pas de temps $dt$ et en considérant que les décisions sont prises en séquence à chaque période de temps $dt$. Alternativement, une approche pour travailler directement en temps continue est présentée à la section \ref{sec:dp_cont}. De plus, pour plusieurs types de problèmes la nature de l'évolution du système est discrètes, par exemple une partie d'échec. La formulation des problèmes en temps discret est donc très générale et s'applique a un grand nombre de problèmes.
      
      Le problème équivalent à résoudre en temps discret est de déterminer les lois de commande $c_k$, qui dictent l'action $u$ à prendre lorsque l'état du système est de $x$ à l'étape $k$:
      
      \begin{align}
      u_k = c_k( x_k )
      \end{align}
      
      de sorte à minimiser un coût additif de la forme:
      
      \begin{align}
          J = \sum_{k=0}^{N-1} g_k(x_k, u_k) + g_N( x_N )
      \end{align}
      où $N$ est l'horizon qui représente ici un nombre d'étape. De plus ici l'évolution du système est représentée par une équation de différence:
      \begin{align}
          x_{k+1} = f_k( x_k, u_k)
      \end{align}
      
      Si on reforme tout le problème en une seule équation mathématique:
      
      \begin{align}
          J^*(x_0) = \min_{c_0, ... c_k, ... c_{N-1}} 
          \left[ \sum_{k=0}^{N-1} g_k(x_k, u_k) + g_N( x_N )
          \right] \quad
          \text{avec} \quad
          x_{k+1} = f_k( x_k, c_k(x_k) )
      \end{align}
      
      on cherche les fonctions $c_k$, i.e. les loi de commandes, qui vont minimiser le coût cumulatif  sur la trajectoire du système, avec l'évolution qui est définit par une dynamique $f_k$ et les lois de commandes $c_k$.
      
    </p>

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Algorithme de programmation dynamique exacte
    </h2>

    <p>
      \begin{align}
      J^*_N(x_N) &= g_N(x_N) \\
      J^*_k(x_k) &= 
      \min_{u_k \in U_k(x_k)}
      \left[
      g_k(x_k , u_k ) + J^*_{k+1}( 
      \underbrace{
      f_k(x_k , u_k ) 
      }_{x_{k+1}}
      )
      \right] \\
      c^*_k(x_k) &= 
      arg\min_{u_k\in U_k(x_k)}
      \left[
      g_k(x_k , u_k ) + J^*_{k+1}( 
      \underbrace{
      f_k(x_k , u_k ) 
      }_{x_{k+1}}
      )
      \right] 
      \label{eq:exactdp}
      \end{align} 
    </p>

    <h2 class="w3-border-bottom w3-border-light-grey w3-padding-16"> 
      Variations sur un thème de programmation dynamique
    </h2>

    <h3>Stochastique</h3>



    \begin{align}
    J^*_k(x_k) = 
    \min_{u_k}
    {\color{red}
    E_{w_k}
    }
    &\left[
    g_k(x_k , u_k , w_k ) + J^*_{k+1}( 
    \underbrace{
    f_k(x_k , u_k , w_k ) 
    }_{x_{k+1}}
    )
    \right] 
    \end{align} 


    <h3>Robuste</h3>


    \begin{align}
    J^*_k(x_k) = 
    \min_{u_k}
    {\color{red}
    \max_{w_k}
    }
    &\left[
    g_k(x_k , u_k , w_k ) + J^*_{k+1}( 
    \underbrace{
    f_k(x_k , u_k , w_k ) 
    }_{x_{k+1}}
    )
    \right] 
    \end{align} 

    <h3>À horizon de temps infini</h3>


    \begin{align}
    J^*(x) = 
    \min_{u}
    &\left[
    g(x , u ) + {\color{red}\alpha} J^*( 
    \underbrace{
    f(x , u ) 
    }_{x_{k+1}}
    )
    \right] 
    \end{align} 


    <h3>Sans modèles (Q-Learning)</h3>


    \begin{align}
    Q^*(x, u ) = g(x , u ) + 
    \min_{u_{k+1}}
    &\left[
    Q^*( 
    \underbrace{
    f(x , u ) 
    }_{x_{k+1}}
    , u_{k+1}
    )
    \right] 
    \end{align} 


    <h3>À temps continu</h3>


    \begin{align}
    0 =
    \min_{u}
    \left[
    g(x , u ) + \frac{\partial	J^*(x,t)}{\partial x }
    \underbrace{
    f(x , u , t) )
    }_{\dot{x}}
    \right]
    \label{eq:hjb}
    \end{align} 


  </div>


</div>

<!-- Footer -->
<footer class="w3-center w3-black w3-padding-16">
  <p>Alexandre Girard | alex.girard@usherbrooke.ca | <a href="https://www.usherbrooke.ca">
  UdeS </a> | <a href="https://www.linkedin.com/in/alx87grd">
  LinkedIn.com/in/alx87grd </a>  </p>
</footer>

</body>
</html>
